"""The objective of this module is to automate the steps to scrape a given publication"""
import os
import subprocess
import scrapy
from loguru import logger

fmt = "{time} - {name} - {level} - {message}"
logger.add("spam.log", level="DEBUG", format=fmt)
logger.add("error.log", level="ERROR", format=fmt)


class Scraper:
    """This class will be broken in the steps needed to create a scraping campaign. Each step a function."""

    def __init__(self, name, project_name, path, start_urls, title, author, date_altered, content):
        self.name = name
        self.project_name = project_name
        self.path = path
        self.start_urls = start_urls
        self.title = title
        self.author = author
        self.date_altered = date_altered
        self.content = content

    def dislocation1(self):
        """We head for designated local for the app and create a folder to house it"""
        os.chdir("/home/mic/python/scraper")
        os.mkdir(f"{self.project_name}")

    def start_project(self):
        """Scrapy command to start a project"""
        path = f"/home/mic/python/scraper/{self.project_name}"
        cmd = "scrapy startproject " + self.name
        subprocess.run(cmd, cwd=path, shell=True)

    def start_spider(self):
        """Scrapy command to initiate a spider"""
        project_name = self.project_name
        name = self.name
        paths = "/home/mic/python/scraper/" + project_name + "/" + name + "/" + name + "/spiders"
        cmd = f"scrapy genspider {self.name}_info {self.start_urls}"  # O nome tem de ser diferente do projecto.
        subprocess.run(cmd, cwd=paths, shell=True)

    def edit_spider_file_clean_file(self):
        """Here we clean the spider file a bit. It comes a bit dirty when it is generated by Scrapy"""
        os.chdir(f"/{self.path}/{self.project_name}/{self.name}/{self.name}/spiders")
        with open(f"{self.name}_info.py", "r") as infile, open(f"{self.name}_output.py", "w") as outfile:
            data = infile.read()
            data = data.replace("http://", "")
            data = data.replace("pass", "")
            outfile.write(data)
            os.remove(f"{self.name}_info.py")
            cmd = f"mv {self.name}_output.py {self.name}_info.py"
            subprocess.run(cmd, shell=True)

    def edit_spider_file_parse_function(self):
        """We add to the file information regarding the areas of the site we want to target"""
        os.chdir(f"/{self.path}/{self.project_name}/{self.name}/{self.name}/spiders")
        with open(f"{self.name}_info.py", "a") as f:
            f.write(f"        title = response.xpath('{self.title}').extract()" "\n")
            f.write(f"        author = response.xpath('{self.author}').extract()" "\n")
            f.write(f"        date_altered = response.xpath('{self.date_altered}').extract()" "\n")
            f.write(f"        content = response.xpath('{self.content}').getall()" "\n")
            f.write("        data = zip(title, author, date_altered, content)")
            f.write("\n")
            f.write("        for item in data:\n")
            f.write(
                "            info = {'title': i:wtem[0], 'author': item[1], 'date_altered': item[2], 'content': item[3]}"
            )
            f.write("\n")
            f.write("        yield info")
            f.write("\n")
            f.write("        with open('content.txt', 'w') as cont:")
            f.write("\n")
            f.write(f"            for i in response.xpath('{self.content}').getall():")
            f.write("\n")
            f.write("                cont.write(str(i))")

    def settings(self):
        """We add information regarding the format of the output"""
        os.chdir(f"{self.path}/{self.project_name}/{self.name}/{self.name}")
        with open("settings.py", "a") as f:
            f.write('FEED_FORMAT = "json"')
            f.write("\n")
            f.write(f"FEED_URI = '{self.name}.json'")
        print("Changes to settings are done")

    def crawl(self):
        """We initiate the scraping"""
        path = self.path
        project_name = self.project_name
        name = self.name
        path = path + "/" + project_name + "/" + name + "/" + name + "/spiders/"
        cmd = "scrapy crawl " + name + "_info"
        subprocess.run(cmd, cwd=path, shell=True)

    def result(self):
        """Verifying the output file, to see if everythng is ok"""
        name = self.name
        path = f"{self.path}/{self.project_name}/{self.name}/{self.name}/spiders/"
        cmd = "vim " + name + ".json"
        subprocess.run(cmd, cwd=path, shell=True)
